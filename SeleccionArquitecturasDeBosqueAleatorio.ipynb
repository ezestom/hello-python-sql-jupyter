{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Ejercicio: bosques aleatorios y arquitectura modelo\n","\n","En el ejercicio anterior, usamos árboles de decisión para predecir si un crimen se resolvería en San Francisco.\n","\n","Recuerde que los árboles de decisión hicieron un trabajo razonable, pero tienden a sobreajustarse, lo que significa que los resultados se degradarían considerablemente al usar el conjunto de prueba o cualquier dato no visto.\n","\n","Esta vez usaremos bosques aleatorios para abordar esa tendencia de sobreajuste.\n","\n","También veremos cómo la arquitectura del modelo puede influir en su rendimiento.\n","\n","Visualización y preparación de datos\n","Como de costumbre, echemos otro vistazo rápido al conjunto de datos del crimen, luego divídalo en conjuntos de entrenamiento y prueba:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas\n","!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n","!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/san_fran_crime.csv\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import graphing # custom graphing code. See our GitHub repo for details\n","\n","# Import the data from the .csv file\n","dataset = pandas.read_csv('san_fran_crime.csv', delimiter=\"\\t\")\n","\n","# Remember to one-hot encode our crime and PdDistrict variables \n","categorical_features = [\"Category\", \"PdDistrict\"]\n","dataset = pandas.get_dummies(dataset, columns=categorical_features, drop_first=False)\n","\n","# Split the dataset in an 90/10 train/test ratio. \n","# Recall that our dataset is very large so we can afford to do this\n","# with only 10% entering the test set\n","train, test = train_test_split(dataset, test_size=0.1, random_state=2, shuffle=True)\n","\n","# Let's have a look at the data and the relationship we are going to model\n","print(dataset.head())\n","print(\"train shape:\", train.shape)\n","print(\"test shape:\", test.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["¡Espero que esto te resulte familiar! Si no, salte hacia atrás y realice el ejercicio anterior sobre árboles de decisión.\n","\n","Código de evaluación del modelo\n","Usaremos el mismo código de evaluación del modelo que usamos en el ejercicio anterior"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import balanced_accuracy_score\n","\n","# Make a utility method that we can re-use throughout this exercise\n","# To easily fit and test out model\n","\n","features = [c for c in dataset.columns if c != \"Resolution\"]\n","\n","def fit_and_test_model(model):\n","    '''\n","    Trains a model and tests it against both train and test sets\n","    '''  \n","    global features\n","\n","    # Train the model\n","    model.fit(train[features], train.Resolution)\n","\n","    # Assess its performance\n","    # -- Train\n","    predictions = model.predict(train[features])\n","    train_accuracy = balanced_accuracy_score(train.Resolution, predictions)\n","\n","    # -- Test\n","    predictions = model.predict(test[features])\n","    test_accuracy = balanced_accuracy_score(test.Resolution, predictions)\n","\n","    return train_accuracy, test_accuracy\n","\n","\n","print(\"Ready to go!\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Árbol de decisión\n","Entrenemos rápidamente un árbol de decisión razonablemente bien ajustado para recordarnos su desempeño:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sklearn.tree\n","# re-fit our last decision tree to print out its performance\n","model = sklearn.tree.DecisionTreeClassifier(random_state=1, max_depth=10) \n","\n","dt_train_accuracy, dt_test_accuracy = fit_and_test_model(model)\n","\n","print(\"Decision Tree Performance:\")\n","print(\"Train accuracy\", dt_train_accuracy)\n","print(\"Test accuracy\", dt_test_accuracy)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Bosque aleatorio\n","\n","Un bosque aleatorio es una colección de árboles de decisión que trabajan juntos para calcular la etiqueta de una muestra.\n","\n","Los árboles en un bosque aleatorio se entrenan de forma independiente, en diferentes particiones de datos y, por lo tanto, desarrollan diferentes sesgos, pero cuando se combinan, es menos probable que sobreajusten los datos.\n","\n","Construyamos un bosque muy simple con dos árboles y los parámetros predeterminados:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# Create a random forest model with two trees\n","random_forest = RandomForestClassifier( n_estimators=2,\n","                                        random_state=2,\n","                                        verbose=False)\n","\n","# Train and test the model\n","train_accuracy, test_accuracy = fit_and_test_model(random_forest)\n","print(\"Random Forest Performance:\")\n","print(\"Train accuracy\", train_accuracy)\n","print(\"Test accuracy\", test_accuracy)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Nuestro bosque de dos árboles lo ha hecho peor que el árbol único en el conjunto de prueba, aunque ha hecho un mejor trabajo en el conjunto de trenes.\n","\n","Hasta cierto punto, esto debería esperarse. Los bosques aleatorios suelen funcionar con muchos más árboles. El simple hecho de tener dos le permitió sobreajustar los datos de entrenamiento mucho mejor que el árbol de decisión original.\n","\n","Alterar el número de árboles.\n","Luego, construyamos varios modelos de bosque, cada uno con una cantidad diferente de árboles, y veamos cómo funcionan:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import graphing\n","\n","# n_estimators states how many trees to put in the model\n","# We will make one model for every entry in this list\n","# and see how well each model performs \n","n_estimators = [2, 5, 10, 20, 50]\n","\n","# Train our models and report their performance\n","train_accuracies = []\n","test_accuracies = []\n","\n","for n_estimator in n_estimators:\n","    print(\"Preparing a model with\", n_estimator, \"trees...\")\n","\n","    # Prepare the model \n","    rf = RandomForestClassifier(n_estimators=n_estimator, \n","                                random_state=2, \n","                                verbose=False)\n","    \n","    # Train and test the result\n","    train_accuracy, test_accuracy = fit_and_test_model(rf)\n","\n","    # Save the results\n","    test_accuracies.append(test_accuracy)\n","    train_accuracies.append(train_accuracy)\n","\n","\n","# Plot results\n","graphing.line_2D(dict(Train=train_accuracies, Test=test_accuracies), \n","                    n_estimators,\n","                    label_x=\"Numer of trees (n_estimators)\",\n","                    label_y=\"Accuracy\",\n","                    title=\"Performance X number of trees\", show=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Las métricas se ven muy bien para el conjunto de entrenamiento, pero no tanto para el conjunto de prueba. Más árboles tendían a ayudar a ambos, pero solo hasta cierto punto.\n","\n","Podríamos haber esperado que la cantidad de árboles resolviera nuestro problema de sobreajuste, ¡pero este no fue el caso! Lo más probable es que el modelo sea simplemente demasiado complejo en relación con los datos, lo que le permite sobreajustarse al conjunto de entrenamiento.\n","\n","Alteración del número mínimo de muestras para el parámetro de división\n","Recuerde que los árboles de decisión tienen un nodo raíz, nodos internos y nodos hoja, y que los dos primeros se pueden dividir en nodos más nuevos con subconjuntos de datos.\n","\n","Si dejamos que nuestro modelo se divida y creemos demasiados nodos, puede volverse cada vez más complejo y comenzar a sobreajustarse.\n","\n","Una forma de limitar esa complejidad es decirle al modelo que cada nodo debe tener al menos una cierta cantidad de muestras; de lo contrario, no se puede dividir en subnodos.\n","\n","En otras palabras, podemos establecer el parámetro min_samples_split del modelo en la menor cantidad de muestras necesarias para que un nodo se pueda dividir.\n","\n","Nuestro valor predeterminado para min_samples_split es solo 2, por lo que los modelos rápidamente se volverán demasiado complejos si ese parámetro no se modifica.\n","\n","Ahora usaremos el modelo de mejor rendimiento anterior, luego lo probaremos con diferentes valores de min_samples_split y compararemos los resultados:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Shrink the training set temporarily to explore this\n","# setting with a more normal sample size\n","full_trainset = train\n","train = full_trainset[:1000] # limit to 1000 samples\n","\n","min_samples_split = [2, 10, 20, 50, 100, 500]\n","\n","# Train our models and report their performance\n","train_accuracies = []\n","test_accuracies = []\n","\n","for min_samples in min_samples_split:\n","    print(\"Preparing a model with min_samples_split = \", min_samples)\n","\n","    # Prepare the model \n","    rf = RandomForestClassifier(n_estimators=20,\n","                                min_samples_split=min_samples,\n","                                random_state=2, \n","                                verbose=False)\n","    \n","    # Train and test the result\n","    train_accuracy, test_accuracy = fit_and_test_model(rf)\n","\n","    # Save the results\n","    test_accuracies.append(test_accuracy)\n","    train_accuracies.append(train_accuracy)\n","\n","\n","# Plot results\n","graphing.line_2D(dict(Train=train_accuracies, Test=test_accuracies), \n","                    min_samples_split,\n","                    label_x=\"Minimum samples split (min_samples_split)\",\n","                    label_y=\"Accuracy\",\n","                    title=\"Performance\", show=True)\n","\n","# Rol back the trainset to the full set\n","train = full_trainset"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Como puede ver arriba, las pequeñas restricciones en la complejidad del modelo, al limitar su capacidad para dividir nodos, reducen la brecha entre el entrenamiento y el rendimiento de la prueba. Si esto es sutil, lo hace sin dañar en absoluto el rendimiento de la prueba.\n","\n","Al limitar la complejidad del modelo, abordamos el sobreajuste, mejorando su capacidad para generalizar y hacer predicciones precisas sobre datos no vistos.\n","\n","Tenga en cuenta que usar min_samples_split=20 nos dio el mejor resultado para el conjunto de prueba y que los valores más altos empeoraron los resultados.\n","\n","Alterar la profundidad del modelo\n","Un método relacionado para limitar los árboles es restringir max_depth. Esto es equivalente a max_ depth que usamos para nuestro árbol de decisiones, anteriormente. Su valor predeterminado es Ninguno, lo que significa que los nodos se pueden expandir hasta que todas las hojas sean puras (todas las muestras tienen la misma etiqueta) o tienen menos muestras que el valor establecido para min_samples_split.\n","\n","Si max_ depth o min_samples_split son más apropiados depende de la naturaleza de su conjunto de datos, incluido su tamaño. Por lo general, necesitamos experimentar para encontrar la mejor configuración. Investiguemos max_ depth como si solo tuviéramos 500 muestras de delitos disponibles para nuestro conjunto de entrenamiento."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Shrink the training set temporarily to explore this\n","# setting with a more normal sample size\n","full_trainset = train\n","train = full_trainset[:500] # limit to 500 samples\n","\n","max_depths = [2, 4, 6, 8, 10, 15, 20, 50, 100]\n","\n","# Train our models and report their performance\n","train_accuracies = []\n","test_accuracies = []\n","\n","for max_depth in max_depths:\n","    print(\"Preparing a model with max_depth = \", max_depth)\n","\n","    # Prepare the model \n","    rf = RandomForestClassifier(n_estimators=20,\n","                                max_depth=max_depth,\n","                                random_state=2, \n","                                verbose=False)\n","    \n","    # Train and test the result\n","    train_accuracy, test_accuracy = fit_and_test_model(rf)\n","\n","    # Save the results\n","    test_accuracies.append(test_accuracy)\n","    train_accuracies.append(train_accuracy)\n","\n","\n","# Plot results\n","graphing.line_2D(dict(Train=train_accuracies, Test=test_accuracies),\n","                    max_depths,\n","                    label_x=\"Maximum depth (max_depths)\",\n","                    label_y=\"Accuracy\",\n","                    title=\"Performance\", show=True)\n","\n","# Rol back the trainset to the full set\n","train = full_trainset"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["El gráfico anterior nos dice que nuestro modelo en realidad se beneficia de un valor más alto para max_depth, hasta el límite de 15.\n","\n","El aumento de la profundidad más allá de este punto comienza a dañar el rendimiento de la prueba, ya que restringe demasiado el modelo para que pueda generalizarse.\n","\n","Como de costumbre, es importante evaluar diferentes valores al establecer los parámetros del modelo y definir su arquitectura.\n","\n","Un modelo optimizado\n","La optimización adecuada de un modelo en un conjunto de datos tan grande puede llevar muchas horas, más de las que necesita para comprometerse con este ejercicio solo para aprender. Si desea ejecutar un modelo que ya se ha optimizado para el conjunto de datos completo, puede ejecutar el código a continuación y comparar su rendimiento con todo lo que hemos visto hasta ahora.\n","\n","Esto es opcional; solo tenga en cuenta que el modelo puede tardar entre 1 y 2 minutos en entrenarse debido a su tamaño y al gran volumen de datos."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Prepare the model \n","rf = RandomForestClassifier(n_estimators=200,\n","                            max_depth=128,\n","                            max_features=25,\n","                            min_samples_split=2,\n","                            random_state=2, \n","                            verbose=False)\n","\n","# Train and test the result\n","print(\"Training model. This may take 1 - 2 minutes\")\n","train_accuracy, test_accuracy = fit_and_test_model(rf)\n","\n","# Print out results, compared to the decision tree\n","data = {\"Model\": [\"Decision tree\",\"Final random forest\"],\n","        \"Train sensitivity\": [dt_train_accuracy, train_accuracy],\n","        \"Test sensitivity\": [dt_test_accuracy, test_accuracy]\n","        }\n","\n","pandas.DataFrame(data, columns = [\"Model\", \"Train sensitivity\", \"Test sensitivity\"])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Como puede ver, el ajuste fino de los parámetros del modelo resultó en una mejora significativa en los resultados del conjunto de prueba.\n","\n","Resumen\n","En este ejercicio cubrimos los siguientes temas:\n","\n","Modelos de bosques aleatorios y en qué se diferencian de los árboles de decisión<br>\n","Cómo podemos cambiar la arquitectura de un modelo estableciendo diferentes parámetros y cambiando sus valores<br>\n","La importancia de probar varias combinaciones de parámetros y evaluar estos cambios para mejorar el rendimiento<br>\n","En el futuro verá que diferentes modelos tienen arquitecturas donde puede ajustar los parámetros. Se necesita experimentación para lograr los mejores resultados posibles."]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
