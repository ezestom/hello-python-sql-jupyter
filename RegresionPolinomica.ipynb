{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio: ajuste de una curva polinomial\n",
    "\n",
    "En este ejercicio, veremos un tipo diferente de regresión llamada regresión polinomial. A diferencia de la regresión lineal que modela las relaciones como líneas rectas, la regresión polinomial modela las relaciones como curvas.\n",
    "\n",
    "Recuerde en nuestro ejercicio anterior cómo la relación entre la temperatura_central y el contenido_proteico_de_la_última_comida no podía explicarse adecuadamente utilizando una línea recta. En este ejercicio, usaremos la regresión polinomial para ajustar una curva a los datos.\n",
    "\n",
    "visualización de datos\n",
    "Comencemos este ejercicio cargando y echando un vistazo a nuestros datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "!pip install statsmodels\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/doggy-illness.csv\n",
    "\n",
    "#Import the data from the .csv file\n",
    "dataset = pandas.read_csv('doggy-illness.csv', delimiter=\"\\t\")\n",
    "\n",
    "#Let's have a look at the data\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresión lineal simple\n",
    "\n",
    "Refresquemos rápidamente nuestra memoria realizando la misma regresión lineal simple que hicimos en el ejercicio anterior usando las columnas temperatura y contenido_proteína_de_la_última_comida del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import graphing # custom graphing code. See our GitHub repo for details\n",
    "\n",
    "# Perform linear regression. This method takes care of\n",
    "# the entire fitting procedure for us.\n",
    "simple_formula = \"core_temperature ~ protein_content_of_last_meal\"\n",
    "simple_model = smf.ols(formula = simple_formula, data = dataset).fit()\n",
    "\n",
    "# Show a graph of the result\n",
    "graphing.scatter_2D(dataset, label_x=\"protein_content_of_last_meal\", \n",
    "                             label_y=\"core_temperature\",\n",
    "                             trendline=lambda x: simple_model.params[1] * x + simple_model.params[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe cómo la relación entre las dos variables no es realmente lineal. Mirando la gráfica, es bastante claro ver que los puntos tienden más hacia un lado de la línea, especialmente para los valores más altos de temperatura central y contenido_proteico_de_la_última_comida. Una línea recta podría no ser la mejor manera de describir esta relación.\n",
    "\n",
    "Echemos un vistazo rápido a la puntuación R-Squared del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R-squared:\", simple_model.rsquared)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esa es una puntuación R-Squared bastante razonable, ¡pero veamos si podemos obtener una aún mejor!\n",
    "\n",
    "Regresión polinomial simple\n",
    "Ajustemos una regresión polinomial simple esta vez. De manera similar a una regresión lineal simple, una regresión polinomial simple modela la relación entre una etiqueta y una sola característica. A diferencia de una regresión lineal simple, una regresión polinomial simple puede explicar relaciones que no son simplemente líneas rectas.\n",
    "\n",
    "En nuestro ejemplo, vamos a utilizar un polinomio de tres parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform polynomial regression. This method takes care of\n",
    "# the entire fitting procedure for us.\n",
    "polynomial_formula = \"core_temperature ~ protein_content_of_last_meal + I(protein_content_of_last_meal**2)\"\n",
    "polynomial_model = smf.ols(formula = polynomial_formula, data = dataset).fit()\n",
    "\n",
    "# Show a graph of the result\n",
    "graphing.scatter_2D(dataset, label_x=\"protein_content_of_last_meal\", \n",
    "                             label_y=\"core_temperature\",\n",
    "                             # Our trendline is the equation for the polynomial\n",
    "                             trendline=lambda x: polynomial_model.params[2] * x**2 + polynomial_model.params[1] * x + polynomial_model.params[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eso ya se ve mucho mejor. Confirmemos echando un vistazo rápido a la puntuación R-Squared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R-squared:\", polynomial_model.rsquared)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esa es una puntuación R-Squared_ mejor que la obtenida con el modelo anterior, ¡excelente! Ahora podemos decirle con confianza a nuestro veterinario que dé prioridad a los perros que comieron una dieta rica en proteínas la noche anterior.\n",
    "\n",
    "Tracemos nuestro modelo como un gráfico 3D. vamos a ver\n",
    "X y X² como dos parámetros separados. Tenga en cuenta que si gira el elemento visual a la derecha, nuestro modelo de regresión sigue siendo un plano. Esta es la razón por la que los modelos polinómicos todavía se consideran modelos lineales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fig = graphing.surface(\n",
    "    x_values=np.array([min(dataset.protein_content_of_last_meal), max(dataset.protein_content_of_last_meal)]),\n",
    "    y_values=np.array([min(dataset.protein_content_of_last_meal)**2, max(dataset.protein_content_of_last_meal)**2]),\n",
    "    calc_z=lambda x,y: polynomial_model.params[0] + (polynomial_model.params[1] * x) + (polynomial_model.params[2] * y),\n",
    "    axis_title_x=\"x\",\n",
    "    axis_title_y=\"x2\",\n",
    "    axis_title_z=\"Core temperature\"\n",
    ")\n",
    "# Add our datapoints to it and display\n",
    "fig.add_scatter3d(x=dataset.protein_content_of_last_meal, y=dataset.protein_content_of_last_meal**2, z=dataset.core_temperature, mode='markers')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrapolando\n",
    "\n",
    "Veamos qué sucede si extraplacamos nuestros datos. Nos gustaría ver si se espera que los perros que comieron alimentos incluso más ricos en proteínas se enfermen aún más.\n",
    "\n",
    "Comencemos con la regresión lineal. Podemos establecer a qué rango nos gustaría extrapolar nuestros datos usando el argumento x_range en la función de trazado. Extrapolemos sobre el rango [0,100]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an extrapolated graph of the linear model\n",
    "graphing.scatter_2D(dataset, label_x=\"protein_content_of_last_meal\", \n",
    "                             label_y=\"core_temperature\",\n",
    "                             # We extrapolate over the following range\n",
    "                             x_range = [0,100],\n",
    "                             trendline=lambda x: simple_model.params[1] * x + simple_model.params[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, extrapolamos la regresión polinomial sobre el mismo rango:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an extrapolated graph of the polynomial model\n",
    "graphing.scatter_2D(dataset, label_x=\"protein_content_of_last_meal\", \n",
    "                             label_y=\"core_temperature\",\n",
    "                             # We extrapolate over the following range\n",
    "                             x_range = [0,100],\n",
    "                             trendline=lambda x: polynomial_model.params[2] * x**2 + polynomial_model.params[1] * x + polynomial_model.params[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Estas dos gráficas predicen dos cosas muy diferentes!\n",
    "\n",
    "La regresión polinómica extrapolada espera que la temperatura central disminuya, mientras que la regresión lineal extrapolada espera que la temperatura central aumente. Una mirada rápida a los gráficos obtenidos en el ejercicio anterior confirma que deberíamos esperar que la temperatura central aumente a medida que aumenta el contenido de proteínas de la última comida, no que disminuya.\n",
    "\n",
    "En general, no se recomienda extrapolar a partir de una regresión polinomial a menos que tenga una razón a priori para hacerlo (que rara vez ocurre, por lo que es mejor pecar de precavido y nunca extrapolar a partir de regresiones polinómicas). )\n",
    "\n",
    "Resumen\n",
    "Cubrimos los siguientes conceptos en este ejercicio:\n",
    "\n",
    "Cree modelos de regresión lineal simple y regresión polinomial simple.<br>\n",
    "Compare el rendimiento de ambos modelos al trazarlos y observar los valores R-Squared.<br>\n",
    "Extrapoló los modelos a un rango más amplio de valores."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
