{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio: Más métricas derivadas de matrices de confusión\n",
    "\n",
    "En este ejercicio aprenderemos sobre diferentes métricas, usándolas para explicar los resultados obtenidos del modelo de clasificación binaria que construimos en la unidad anterior.\n",
    "\n",
    "Visualización de datos\n",
    "Usaremos el conjunto de datos con diferentes clases de objetos encontrados en la montaña una vez más:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/snow_objects.csv\n",
    "\n",
    "#Import the data from the .csv file\n",
    "dataset = pandas.read_csv('snow_objects.csv', delimiter=\"\\t\")\n",
    "\n",
    "#Let's have a look at the data\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerde que para usar el conjunto de datos anterior para la clasificación binaria, debemos agregar otra columna al conjunto de datos y configurarlo en Verdadero donde la etiqueta original es excursionista y Falso donde no lo es.\n",
    "\n",
    "Luego agreguemos esa etiqueta, dividamos el conjunto de datos y entrenemos el modelo nuevamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add a new label with true/false values to our dataset\n",
    "dataset[\"is_hiker\"] = dataset.label == \"hiker\"\n",
    "\n",
    "# Split the dataset in an 70/30 train/test ratio. \n",
    "train, test = train_test_split(dataset, test_size=0.3, random_state=1, shuffle=True)\n",
    "\n",
    "# define a random forest model\n",
    "model = RandomForestClassifier(n_estimators=1, random_state=1, verbose=False)\n",
    "\n",
    "# Define which features are to be used \n",
    "features = [\"size\", \"roughness\", \"motion\"]\n",
    "\n",
    "# Train the model using the binary label\n",
    "model.fit(train[features], train.is_hiker)\n",
    "\n",
    "print(\"Model trained!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos usar este modelo para predecir si los objetos en la nieve son excursionistas o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn has a very convenient utility to build confusion matrices\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Calculate the model's accuracy on the TEST set\n",
    "actual = test.is_hiker\n",
    "predictions = model.predict(test[features])\n",
    "\n",
    "# Build and print our confusion matrix, using the actual values and predictions \n",
    "# from the test set, calculated in previous cells\n",
    "cm = confusion_matrix(actual, predictions, normalize=None)\n",
    "\n",
    "# Create the list of unique labels in the test set, to use in our plot\n",
    "# I.e., ['True', 'False',]\n",
    "unique_targets = sorted(list(test[\"is_hiker\"].unique()))\n",
    "\n",
    "# Convert values to lower case so the plot code can count the outcomes\n",
    "x = y = [str(s).lower() for s in unique_targets]\n",
    "\n",
    "# Plot the matrix above as a heatmap with annotations (values) in its cells\n",
    "fig = ff.create_annotated_heatmap(cm, x, y)\n",
    "\n",
    "# Set titles and ordering\n",
    "fig.update_layout(  title_text=\"<b>Confusion matrix</b>\", \n",
    "                    yaxis = dict(categoryorder = \"category descending\"))\n",
    "\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=0.5,\n",
    "                        y=-0.15,\n",
    "                        showarrow=False,\n",
    "                        text=\"Predicted label\",\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=-0.15,\n",
    "                        y=0.5,\n",
    "                        showarrow=False,\n",
    "                        text=\"Actual label\",\n",
    "                        textangle=-90,\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "# We need margins so the titles fit\n",
    "fig.update_layout(margin=dict(t=80, r=20, l=120, b=50))\n",
    "fig['data'][0]['showscale'] = True\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also calculate some values that will be used throughout this exercise\n",
    "# We already have actual values and corresponding predictions, defined above\n",
    "correct = actual == predictions\n",
    "tp = numpy.sum(correct & actual)\n",
    "tn = numpy.sum(correct & numpy.logical_not(actual))\n",
    "fp = numpy.sum(numpy.logical_not(correct) & actual)\n",
    "fn = numpy.sum(numpy.logical_not(correct) & numpy.logical_not(actual))\n",
    "\n",
    "print(\"TP - True Positives: \", tp)\n",
    "print(\"TN - True Negatives: \", tn)\n",
    "print(\"FP - False positives: \", fp)\n",
    "print(\"FN - False negatives: \", fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar los valores y la matriz anteriores para ayudarnos a comprender otras métricas.\n",
    "\n",
    "Cálculo de métricas\n",
    "De aquí en adelante, analizaremos más de cerca cada una de las siguientes métricas, cómo se calculan y cómo pueden ayudar a explicar nuestro modelo actual.\n",
    "\n",
    "Exactitud<br>\n",
    "Sensibilidad/recuperación<br>\n",
    "especificidad<br>\n",
    "Precisión<br>\n",
    "Tasa de falsos positivos<br>\n",
    "Primero recordemos algunos términos útiles:<br>\n",
    "\n",
    "TP = Verdaderos positivos: una etiqueta positiva se predice correctamente<br>\n",
    "TN = Verdaderos negativos: una etiqueta negativa se predice correctamente<br>\n",
    "FP = Falsos positivos: una etiqueta negativa se predice como positiva<br>\n",
    "FN = Falsos negativos: una etiqueta positiva se predice como negativa<br>\n",
    "Exactitud<br>\n",
    "La precisión es el número de predicciones correctas dividido por el número total de predicciones:<br>\n",
    "\n",
    "   accuracy = (TP+TN) / number of samples\n",
    "\n",
    "It's possibly the most basic metric used but, as we've seen, it's not the most reliable when imbalanced datasets are used.\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for sensitivity/recall\n",
    "sensitivity = recall = tp / (tp + fn)\n",
    "\n",
    "# print result as a percentage\n",
    "print(f\"Model sensitivity/recall is {sensitivity:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "especificidad\n",
    "\n",
    "La especificidad expresa la fracción de etiquetas negativas predichas correctamente sobre el número total de muestras negativas existentes:\n",
    "\n",
    "     especificidad = TN / (TN + FP)\n",
    "     \n",
    "Nos dice cómo, de todas las muestras realmente negativas, cuántas se predicen correctamente como negativas\n",
    "\n",
    "Se puede calcular con el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for specificity\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# print result as a percentage\n",
    "print(f\"Model specificity is {specificity:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisión\n",
    "La precisión expresa la proporción de muestras positivas pronosticadas correctamente sobre todas las predicciones positivas:\n",
    "\n",
    "     precisión = TP / (TP + FP)\n",
    "     \n",
    "En otras palabras, indica cómo de todas las predicciones positivas, cuántas son etiquetas verdaderamente positivas.\n",
    "\n",
    "Se puede calcular con el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for precision\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "\n",
    "# print result as a percentage\n",
    "print(f\"Model precision is {precision:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasa de falsos positivos\n",
    "Tasa de falsos positivos o FPR, es el número de predicciones positivas incorrectas dividido por el número total de muestras negativas:\n",
    "\n",
    "     tasa_falso_positivo = FP / (FP + TN)\n",
    "     \n",
    "De todos los negativos reales, ¿cuántos se clasificaron erróneamente como positivos?\n",
    "\n",
    "En codigo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for false positive rate\n",
    "false_positive_rate = fp / (fp + tn)\n",
    "\n",
    "# print result as a percentage\n",
    "print(f\"Model false positive rate is {false_positive_rate:.2f}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenga en cuenta que la suma de la especificidad y la tasa de falsos positivos siempre debe ser igual a 1.\n",
    "\n",
    "Conclusión\n",
    "Hay varias métricas diferentes que pueden ayudarnos a evaluar el rendimiento de un modelo, en el contexto de la calidad de sus predicciones.\n",
    "\n",
    "La elección de las métricas más adecuadas, sin embargo, es principalmente una función de los datos y del problema que estamos tratando de resolver.\n",
    "\n",
    "Resumen\n",
    "Cubrimos los siguientes temas en esta unidad:\n",
    "\n",
    "Cómo calcular las medidas muy básicas utilizadas en la evaluación de modelos de clasificación: TP, FP, TN, FN.<br>\n",
    "Cómo usar las medidas anteriores para calcular métricas más significativas, como:<br>\n",
    "Exactitud<br>\n",
    "Sensibilidad/recuperación<br>\n",
    "especificidad<br>\n",
    "Precisión<br>\n",
    "Tasa de falsos positivos<br>\n",
    "Cómo la elección de las métricas depende del conjunto de datos y del problema que estamos tratando de resolver."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
